<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>matt williams</title>
    <link href="https://mattwilliams-ds.github.io/gh-page/feed.xml" rel="self" />
    <link href="https://mattwilliams-ds.github.io/gh-page" />
    <updated>2026-02-07T10:53:24-07:00</updated>
    <author>
        <name>Matt Williams</name>
    </author>
    <id>https://mattwilliams-ds.github.io/gh-page</id>

    <entry>
        <title>Pairwise Radius Clustering of Gantry Kinematics</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/pairwise-radius-clustering/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/pairwise-radius-clustering/</id>
            <category term="Engineering"/>
            <category term="All Projects"/>

        <updated>2025-11-29T11:46:41-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept I was recently tasked with reviewing several hundred launching girder kinematic manuals. Launching girders are large overhead cranes used to erect bridges and the kinematic manuals are step-by-step procedures for erecting each bridge span. To economize our efforts I needed a way to group&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>I was recently tasked with reviewing several hundred launching girder kinematic manuals. Launching girders are large overhead cranes used to erect bridges and the kinematic manuals are step-by-step procedures for erecting each bridge span.</p>
<figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/13//lg.png" alt="launching girder" width="1000" height="750" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/lg-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/lg-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/lg-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/lg-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/lg-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/lg-2xl.png 1920w"></figure>
<p>To economize our efforts I needed a way to group these manuals and thus reduce the number of sequences we had to perform a full structural analysis of.</p>
<p>Two manuals can be considered similar if the length of the span the gantry starts at and the length of the span that the gantry launches to are similar. For this exercise, two spans are considered similar if they are within 5m in length of each other.</p>
<p>Since both span lengths are in meters and similar spans are within 5m of each other, running the clustering algorithm with a radius of 2.5m resulted in appropriate clusters of like manuals.</p>
<p><strong>Data</strong></p>
<p>The data used included the length of the two span lengths of each kinematic manual as well as a label denoting which launching gantry was used to erect each span. This data was specific to the project I was working on and is not publicly available.</p>
<p><strong>Process</strong></p>
<ol>
<li>Read the data into a <strong>Pandas</strong> DataFrame in <strong>Python</strong>.</li>
<li>Separate data for each gantry type into separate DataFrames.</li>
<li>Create a <a href="https://contrib.scikit-learn.org/radius_clustering/index.html" title="radius clustering library" target="_blank" rel="noopener noreferrer">Radius Cluster</a> model for each gantry type with a specified radius to consider.</li>
<li>Apply clustering group numbers to each data point and export for further analysis in <strong>Excel</strong>.</li>
<li>Use engineering judgement to further refine the groups.</li>
</ol>
<p><strong>Results</strong></p>
<p>The following plot shows the 1st and 2nd span lengths for the kinematic manuals of one of the launching girders. They are colored by the assigned group number. The clustering algorithm assigned 27 groups in total though the plot is limited by the number of colors in the color scale.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/13//l160_groups.png" alt="kinematic manual clustering" width="563" height="453" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/l160_groups-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/l160_groups-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/l160_groups-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/l160_groups-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/l160_groups-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/13//responsive/l160_groups-2xl.png 1920w"></figure>
<p>The number of groups was further reduced using engineering judgement and drastically improved our workflow.</p>
<p>My python code is available <a href="https://github.com/mattwilliams-ds/Data_Science_Projects/blob/main/radius_clustering/EM_grouping_v02.ipynb" target="_blank" rel="noopener noreferrer">on github here</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Vehicle Crash Density Map</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/vehicle-crash-density-map/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/vehicle-crash-density-map/</id>
            <category term="Engineering"/>
            <category term="All Projects"/>

        <updated>2025-11-29T08:03:59-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept After being rear-ended in a car accident, I wanted to analyze which intersections in the town where the accident occurred have experienced the highest number of car accidents over the past four years. Data Colorado Department of Transportation Crash Data Process Results Here is&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>After being rear-ended in a car accident, I wanted to analyze which intersections in the town where the accident occurred have experienced the highest number of car accidents over the past four years.</p>
<p><strong>Data</strong></p>
<p><a href="https://www.codot.gov/safety/traffic-safety/data-analysis/crash-data" title="CDOT Crash Data" target="_blank" rel="noopener noreferrer">Colorado Department of Transportation Crash Data</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Import crash data to a <strong>Pandas</strong> DataFrame for 2021 through 2024.</li>
<li>Concatenate all crash data.</li>
<li>Clean data &amp; standardizing street names.</li>
<li>Create a single column containing both cross-streets for each accident.</li>
<li>Use the <strong>Google Maps API</strong> to geocode all intersections.</li>
<li>Export intersections and accident count data to a CSV &amp; import as a <strong>SQL</strong> database.</li>
<li>Write a <a href="https://github.com/mattwilliams-ds/Data_Science_Projects/blob/main/longmont_accidents/longmont_crashes.sql" title="SQL query" target="_blank" rel="noopener noreferrer">SQL query</a> to compile a list of intersections and accident counts.</li>
<li>Use <strong>QGIS</strong> to create a map of the intersections with the highest accident counts.</li>
</ol>
<p><strong>Results</strong></p>
<p>Here is the map I created of the twenty intersections with the highest accident counts. And, yes, the intersection where my accident occured made the top twenty.</p>
<figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/12/Longmont_Crash_Density.png" alt="Crash Density Map of Longmont, Colorado" width="1166" height="1078" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/12/responsive/Longmont_Crash_Density-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/12/responsive/Longmont_Crash_Density-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/12/responsive/Longmont_Crash_Density-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/12/responsive/Longmont_Crash_Density-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/12/responsive/Longmont_Crash_Density-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/12/responsive/Longmont_Crash_Density-2xl.png 1920w"></figure>
<p>All of my project files can be viewed <a href="https://github.com/mattwilliams-ds/Data_Science_Projects/tree/main/longmont_accidents" title="github link" target="_blank" rel="noopener noreferrer">here on github</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Vulnerability Study of Coastal Bridges</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/vulnerability-study-of-coastal-bridges/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/vulnerability-study-of-coastal-bridges/</id>
            <category term="Engineering"/>
            <category term="All Projects"/>

        <updated>2025-01-19T13:13:22-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept This project sought to develop a system to rate the vulnerability of bridge infrastructure by coastal counties in the U.S. The scoring system developed awards one level of risk for each of the following factors: By identifying coastal counties with at risk bridge structures,&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project sought to develop a system to rate the vulnerability of bridge infrastructure by coastal counties in the U.S. The scoring system developed awards one level of risk for each of the following factors:</p>
<ul>
<li><figure class="post__image post__image--right"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/10/median_bridge_age-2.png" alt="Median Bridge Age" width="400" height="226" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-2xl.png 1920w"></figure>The bridge population within the county is aging</li>
<li>Bridge Ratings are in decline</li>
<li>County population is increasing (putting more demand on the bridges)</li>
<li>Frequency of storm events that could impact structure health are increasing</li>
</ul>
<p>By identifying coastal counties with at risk bridge structures, Federal and state transportation agencies can have a clear and concise indicator of where future infrastructure investments may be needed.</p>
<p><strong>Data</strong></p>
<ol>
<li>FHWA's <a href="https://www.fhwa.dot.gov/bridge/nbi.cfm" title="FHWA National Bridge Inventory" target="_blank" rel="noopener noreferrer">National Bridge Inventory</a></li>
<li><a href="https://www.census.gov/data/datasets/time-series/demo/popest/2020s-counties-total.html" title="US Census Data" target="_blank" rel="noopener noreferrer">US Census data</a></li>
<li>NCEI <a href="https://www.ncdc.noaa.gov/stormevents/ftp.jsp" title="Storm Events Database" target="_blank" rel="noopener noreferrer">Storm Events Database</a></li>
</ol>
<p><strong>Process</strong></p>
<ol>
<li>Clean &amp; process National Bridge Inventory data in <strong>Python</strong>, filtering out non-coastal counties. Attributes of interest included bridge ratings, year of construction, and geolocation data.</li>
<li>Determine bridge age for all structures in cleaned dataset.</li>
<li>Calculate average bridge rating by taking the geometric mean of the ratings for each bridge component.</li>
<li>Calculate the rate of change of the average bridge ratings by county using a <strong>linear regressor</strong> and taking the slope as the rate of change.</li>
<li>Clean &amp; process US Census population data in <strong>Python</strong>.</li>
<li>Calculate rate of population change over a ten year period.</li>
<li>Clean &amp; process NCEI storm event counts, filtering out event types that are not likely to impact bridge condition (fog for example).</li>
<li>Calculate the rate of change of storm event occurrences over time using a <strong>Poisson regressor</strong> and filtering out counties with that are not significant (p-values less than 0.05).</li>
<li>Combine average bridge age, change in average bridge rating, county population, and rate of change of storm events for each county into a single <strong>Pandas</strong> dataframe.</li>
<li>Total all risk factors for each county to determine individual vulnerability scores.</li>
<li>Import county vulnerability score data into <strong>QGIS</strong> and create <strong>chloropleth maps</strong> to document findings.</li>
</ol>
<p><strong>Results</strong></p>
<p>This study successfully rated each coastal county in the US. Ten counties were found to have a vulnerability score of four meaning that they have all for risk factors; aging infrastructure, declining infrastructure, increasing population, and increasing storm events. The following chloropleth map shows the rating for each county.</p>
<figure><figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/10/county_vulnerability_scores.png" alt="County Vulnerability Map" width="1546" height="891" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-2xl.png 1920w"></figure>
<figcaption><em>Vulnerability Scores of Coastal U.S. Counties</em></figcaption>
</figure>
<p>The 10 counties with a vulnerability score of four are:</p>
<table style="border-collapse: collapse; width: 100%; height: 241.75px; border-style: none;" border="0">
<tbody>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">San Diego County, California</td>
<td style="width: 49.943%; height: 48.35px;">Barnstable County, Massachusetts</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Santa Cruz County, California</td>
<td style="width: 49.943%; height: 48.35px;">Douglas County, Oregon</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Fairfield County, Connecticut</td>
<td style="width: 49.943%; height: 48.35px;">Nueces County, Texas</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Escambia County, Florida</td>
<td style="width: 49.943%; height: 48.35px;">James City County, Virginia</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Lee County, Florida</td>
<td style="width: 49.943%; height: 48.35px;">Snohomish County, Washington</td>
</tr>
</tbody>
</table>
<p>The collection of scripts developed in the course of this project are available <a href="https://github.com/mattwilliams-ds/coastal_bridges" title="Bridge Vulnerability Study" target="_blank" rel="noopener noreferrer">here on github</a>. A full report on the project is also <a href="https://drive.google.com/file/d/1Co_J1ejcm1PjDSXzqr7fspOCN_hlEUKl/view?usp=drive_link" title="Bridge Vulnerability Report" target="_blank" rel="noopener noreferrer">available here</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Patterns in Structurally Deficient Bridges</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/patterns-in-structurally-deficient-bridges/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/patterns-in-structurally-deficient-bridges/</id>
            <category term="Engineering"/>
            <category term="All Projects"/>

        <updated>2025-01-11T13:50:40-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept The impetus of this project was to identify common traits among structurally deficient bridges in the United States. To do this, FHWA's National Bridge Inventory was analyzed with the FPGrowth algorithm which is commonly used for finding patterns in retail shopping carts. Applying FPGrowth&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>The impetus of this project was to identify common traits among structurally deficient bridges in the United States. To do this, FHWA's National Bridge Inventory was analyzed with the FPGrowth algorithm which is commonly used for finding patterns in retail shopping carts.</p>
<p>Applying FPGrowth to the bridge inventory required numeric ratings to be categorized as the FPGrowth algorithm works by making associations between text string values present for each record. Several other attributes were also categorized to make more generic rules.</p>
<p><strong>Data</strong></p>
<ul>
<li>FHWA's <a href="https://www.fhwa.dot.gov/bridge/nbi.cfm" title="FHWA National Bridge Inventory" target="_blank" rel="noopener noreferrer">National Bridge Inventory</a></li>
</ul>
<p><strong>Process</strong></p>
<ol>
<li>Read NBI data into a <strong>PySpark</strong> dataframe.</li>
<li>Clean the data by removing bridges with missing or misformatted ratings as well as bridges that are not structurally deficient.</li>
<li>Remove culverts from the data set as these have a different structural system than conventional bridges.</li>
<li>Select data using PySpark <strong>SQL</strong> Functions and create visualizations using <strong>Matplotlib</strong>.</li>
<li>Categorize bridge age, regional location, structure type, and structural system ratings (deck, superstructure, substructure) as follows:<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/9/bridge_rating_categorization.png" alt="Bridge Rating Categorization" width="363" height="240" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-2xl.png 1920w"></figure></li>
<li>Use the <strong>FPGrowth</strong> frequent pattern algorithm to develop association rules.</li>
<li>Review the rules and their support &amp; confidence statistical parameters to determine which are worth highlighting.</li>
<li>Perform additional statistical analyses to identify commonalities common shortcomings of deficient bridges.</li>
</ol>
<p><strong>Results</strong></p>
<p>The most relevant and interesting rules found in the set of structurally deficient bridges are as follows:</p>
<table style="border-collapse: collapse; width: 100%;" border="1">
<tbody>
<tr>
<td class="align-center" style="width: 10.1255%;"><strong>Rule</strong></td>
<td class="align-center" style="width: 29.093%;"><strong>Antecedent</strong></td>
<td class="align-center" style="width: 27.6669%;"><strong>Consequent</strong></td>
<td class="align-center" style="width: 15.2596%;"><strong>Support</strong></td>
<td class="align-center" style="width: 17.8266%;"><strong>Confidence</strong></td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">1</td>
<td style="width: 29.093%;">Deck in Good Condition</td>
<td style="width: 27.6669%;">Substructure in Poor Condition</td>
<td class="align-center" style="width: 15.2596%;">0.73</td>
<td class="align-center" style="width: 17.8266%;">0.43</td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">2</td>
<td style="width: 29.093%;">Superstructure in Good Condition</td>
<td style="width: 27.6669%;">Substructure in Poor Condition</td>
<td class="align-center" style="width: 15.2596%;">0.77</td>
<td class="align-center" style="width: 17.8266%;">0.40</td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">3</td>
<td style="width: 29.093%;">Deck is cast-in-place concrete</td>
<td style="width: 27.6669%;">Deck is in poor condition</td>
<td class="align-center" style="width: 15.2596%;">0.44</td>
<td class="align-center" style="width: 17.8266%;">0.24</td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">4</td>
<td style="width: 29.093%;">Girder bridge type + 1-2 spans long</td>
<td style="width: 27.6669%;">Superstructure is a steel girder type</td>
<td class="align-center" style="width: 15.2596%;">0.75</td>
<td class="align-center" style="width: 17.8266%;">0.25</td>
</tr>
</tbody>
</table>
<p>To give you an idea of what all of this is saying, let's break down the first rule. Starting with the confidence, the rule is telling us that "out of all structurally deficient bridges that have a deck in good condition, 73% of them have a substructure that is in poor condition". The confidence tell us that "43% of all structurally deficient bridges have both a deck that is in good condition and a substructure that is in poor condition".</p>
<p>Note that rules one and two both speak to the substructure being in poor condition. This was one of the major findings of this project. The bridge substructure is the most common structural element in poor condition among structurally deficient bridges. This was confirmed by looking at the percentage of structurally deficient systems in the set of deficient bridges.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/9/poor_structural_elements-2.png" alt="Structural systems in poor conditions." width="744" height="410" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-2xl.png 1920w"></figure>
<p>Here is a video presentation I made of this project. You can also have a look through my <a href="https://colab.research.google.com/drive/1s6ki2vER671L1qWRRjx5AuH6KnHHPdXa?usp=drive_link" title="Colab Notebook" target="_blank" rel="noopener noreferrer">Colab Notebook</a>.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/vI6ARHd0JPk?si=QFAVg02O1kB01pre" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Predicting Concrete Strength</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/predicting-concrete-strength/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/predicting-concrete-strength/</id>
            <category term="Engineering"/>
            <category term="All Projects"/>

        <updated>2025-01-05T09:25:27-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept This project is an extension to my project titled Effects of Concrete Mix Design &amp; Aging on Strength. Here, I sought to develop a machine learning model that could predict the 28 day concrete breaking strength based on the sample's mix design. I used&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project is an extension to my project titled <a href="https://mattwilliams-ds.github.io/gh-page/effects-of-concrete-mix-design-and-aging-on-strength/" title="effects of concrete mix on strength" target="_blank" rel="noopener noreferrer">Effects of Concrete Mix Design &amp; Aging on Strength</a>. Here, I sought to develop a machine learning model that could predict the 28 day concrete breaking strength based on the sample's mix design. I used several regression models predict concrete strengths. I then tuned the best performing model to maximize model results.</p>
<p><strong>Data</strong></p>
<p><a href="https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength" title="UC Irvine concrete compressive data" target="_blank" rel="noopener noreferrer">Concrete Compressive Strength [creative commons] from UC Irvine</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Read data into a <strong>Pandas</strong> dataframe.</li>
<li>Look for &amp; remove records with missing data.</li>
<li>Explore the relationship of the mix ingredients using a <strong>seaborn</strong> correlation matrix and pair plots.</li>
<li>Isolate records for 28 day breaks.</li>
<li>Split the data 85% for training and 15% for testing.</li>
<li>Train &amp; test several <strong>scikit-learn</strong> regression models including decision trees, random forests, kNN, multi-layer perceptron, and adaBoost.</li>
<li>Tune best performing model.</li>
</ol>
<p><strong>Results</strong></p>
<p>The correlation matrix (shown here) highlighted several interesting relationships. Most notably is the negative correlation between water and the superplasticizer additive. Second to that, the cement content and concrete strength showed a positive correlation.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/8/Screenshot-from-2025-01-05-09-50-15.png" alt="seaborn correlation matrix plot" width="600" height="492" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-2xl.png 1920w"></figure>
<p>After training and testing each of the machine learning algorithms, I found that the random forest model performed the best with a model score of 0.839. With further tuning of the model I was able to increase the model performance to 0.891. This was accomplished through feature selection, increasing the depth of the decision trees making up the forest, and increasing the number of trees in the forest.</p>
<p>The following diagram plots the actual breaking strength versus the predicted breaking strength of the test data.</p>
<figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/8/predicting_conc_strength.png" alt="Actual vs Predicted Concrete Strength" width="724" height="738" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/predicting_conc_strength-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/predicting_conc_strength-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/predicting_conc_strength-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/predicting_conc_strength-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/predicting_conc_strength-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/predicting_conc_strength-2xl.png 1920w"></figure>
<p>Check out the entire project <a href="https://github.com/mattwilliams-ds/Data_Science_Projects/blob/main/predicting_concrete_strength/Predicting_Concrete_Strength.ipynb" title="predicting concrete strength notebook" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a> for more information.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Effects of Concrete Mix Design &amp; Aging on Strength</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/effects-of-concrete-mix-design-and-aging-on-strength/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/effects-of-concrete-mix-design-and-aging-on-strength/</id>
            <category term="Engineering"/>
            <category term="All Projects"/>

        <updated>2025-01-04T09:08:24-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept Construction industries the world over favor different strength concretes for different applications. This project seeks to discern patterns in concrete mix component concentrations for low, medium, and high strength concretes. The dataset includes breaking strengths for just over 1,000 samples, their mix design, and the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>Construction industries the world over favor different strength concretes for different applications. This project seeks to discern patterns in concrete mix component concentrations for low, medium, and high strength concretes. The dataset includes breaking strengths for just over 1,000 samples, their mix design, and the age at which the sample was tested.</p>
<p>While the dataset contains samples broken at different ages, the standard age for concrete strengths is 28 days. This is the strength at which most concrete structures are designed with. Concrete strength does increase over time beyond 28 days. This increase, however, is generally not counted on by structural engineers and is why this project uses only the breaks made at 28 days.</p>
<p><strong>Data</strong></p>
<p><a href="https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength" title="UC Irvine concrete compressive data" target="_blank" rel="noopener noreferrer">Concrete Compressive Strength [creative commons] from UC Irvine</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Read csv data into a <strong>Pandas</strong> dataframe.</li>
<li>Isolate 28 day breaking strengths.</li>
<li>Classify 28 day records as having high, medium, or low strength.</li>
<li>Using <strong>Seaborn</strong>, create a series of box plots for each mix ingredient for each breaking strength classification.</li>
<li>Review box plots for trends in mix concentrations across the three breaking strength classes.</li>
</ol>
<p><strong>Results</strong></p>
<p>Below are the box plots showing the ranges of mix component concentration from low strength to high strength concrete strengths, all at 28 days.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/7//Screenshot-from-2025-01-04-09-07-03.png" alt="Box Plots of Concrete Mix Designs" width="1000" height="428" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-2xl.png 1920w"></figure>
<p>While there is a lot of information in the box plots above it is the level of detail a structural engineer would like to see.  Going by component, the cement content increases with strength. This makes sense as cement is the binding agent that holds the aggregates together. As the cement content increases it displaces and thus reduces the aggregate content. Finally, the water content descreases slightly while superplasticizers increased. This too makes sense as superplasticizers are used to reduce the amount of water in the mix while maintaining the ability to work and consolidate the concrete.</p>
<p>The entire project can be viewed in this <a href="https://github.com/mattwilliams-ds/Data_Science_Projects/blob/main/concrete_mix_aging/MWilliams_Concrete_Strength.ipynb" title="Jupyter Notebook" target="_blank" rel="noopener noreferrer">Jupyter Notebook</a>. It also includes some additional analysis to understand strength gains after 28 days as well. The study found that, on average, concrete strength increases by 21% from day 28 to day 365. Again, this is not generally relied upon in design but is useful to know as it provides an additional factor of safety on the design.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Tornadoes in Tableau</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/tornadoes-in-tableau/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/tornadoes-in-tableau/</id>
            <category term="Science"/>
            <category term="All Projects"/>

        <updated>2025-01-03T07:00:08-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept This project was done as part of a data visualization class. My goal with this project was to create dashboards and visualizations in Tableau to show the recent history and impact of tornadoes in the United States. Data NOAA's Storm Events Database Process Results&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project was done as part of a data visualization class. My goal with this project was to create dashboards and visualizations in Tableau to show the recent history and impact of tornadoes in the United States.</p>
<p><strong>Data</strong></p>
<p><a href="https://www.ncdc.noaa.gov/stormevents/" title="NOAA Storm Events Database" target="_blank" rel="noopener noreferrer">NOAA's Storm Events Database</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Reduce storm data to only tornado events.</li>
<li>Clean tornado event data in a <strong>spreadsheet</strong>.</li>
<li>Consolidate storm data to a single event (one storm may be represented as multiple events in the database as it traverses county boundaries)</li>
<li>Develop data visualizations in <strong>Tableau</strong>.</li>
</ol>
<p><strong>Results</strong></p>
<p>This project covers a lot of ground and to communicate all of that several visualizations have been created in Tableau, including a dashboard with a map of all of the tornadoes in this study. Below is a demonstration of my Tableau dashboard and interactive data visualizations.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/R4Rs-IW-sno" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Wireless Sensor Network</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/wireless-sensor-network/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/wireless-sensor-network/</id>
            <category term="Science"/>
            <category term="All Projects"/>

        <updated>2025-01-01T11:36:40-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept An Arduino Uno was used to run a sensor for collecting ambient temperature and humidity readings as well. This data was also output to a serial port that fed into a laptop. Hardware Process Results This sensor network developed for this project worked as&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<figure class="post__image post__image--right"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/5/wireless_sensor_network.png" alt="" width="550" height="413" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-2xl.png 1920w"></figure>The purpose of this project was to develop a three node wireless sensor network with an end node, router, and coordinator. Soil moisture sensors were attached to the end node and router. The data flows from the end node to the router and then from the router to the coordinator. The coordinator would then write all data to a serial port where the laptop would listen and record the incoming data.</p>
<p>An Arduino Uno was used to run a sensor for collecting ambient temperature and humidity readings as well. This data was also output to a serial port that fed into a laptop.</p>
<p><strong>Hardware</strong></p>
<ol>
<li>DIGI XBee3 Zigbee Radio Development Boards</li>
<li>Soil Moisture Sensors</li>
<li>Ardiuno Uno</li>
<li>DHT11 Temperature &amp; Humidity Sensor</li>
</ol>
<p><strong>Process</strong></p>
<ol>
<li>Develop <strong>MicroPython</strong> scripts to establish communication &amp; run sensor data collection on the XBee3 radios.</li>
<li>Develop an <strong>Arduino</strong> script to collect ambient temperature &amp; humidity readings.</li>
<li>Develop a <strong>Python</strong> program that listens to local laptop ports for incoming data from the XBee3 coordinator and the Arduino Uno.</li>
<li>Record all data on the laptop to a CSV file for post processing.</li>
</ol>
<p><strong>Results</strong></p>
<p>This sensor network developed for this project worked as intended. Using try/except clauses I was able to make the network self-healing. Whenever a node dropped off the network, it would restart the communication protocol to locate the node it is supposed to report to. The MicroPython, Arduino, and serial listening scripts are <a href="https://github.com/mattwilliams-ds/sensor-network" target="_blank" rel="noopener noreferrer">available on Github here</a>. Also, check out a demonstration of the network below.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/Yaj0-c3g5m0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Assessing Home Buyer Risk</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/assessing-homebuyers-risk/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/assessing-homebuyers-risk/</id>
            <category term="Business Intelligence"/>
            <category term="All Projects"/>

        <updated>2024-12-31T15:43:33-07:00</updated>
            <summary type="html">
                <![CDATA[
                    A web application for evaluating property risks in Boulder County, Colorado. Concept The web map, created with ESRI's ArcGIS Web Application, allows homebuyers and residents to locate a property and see nearby risks including fracking wells, FEMA flood zones, Metro taxing district boundaries, as well&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>A web application for evaluating property risks in Boulder County, Colorado.</p>
<p><strong>Concept</strong></p>
<figure class="post__image post__image--right"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/3/webapp-2.png" alt="" width="450" height="356" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-2xl.png 1920w"></figure>There are a lot of tools for assessing a homes proximity to schools, community amenities, parks, and more. What is harder to come by are tools for assessing potential risks to a home you may wish to purchase. This project seeks to solve that problem for Boulder County, Colorado.</p>
<p>The web map, created with ESRI's ArcGIS Web Application, allows homebuyers and residents to locate a property and see nearby risks including fracking wells, FEMA flood zones, Metro taxing district boundaries, as well as wildfire risk. Users of the application can then gauge their own comfort level with the proximity to these risk factors.</p>
<p><strong>Data</strong></p>
<ol>
<li>Fracking well locations and current status from <a href="https://www.fractracker.org/" target="_blank" rel="noopener noreferrer">FracTracker Alliance</a></li>
<li><a href="https://geo.colorado.edu/catalog/47540-5ca228ffd43267000b8c7448" target="_blank" rel="noopener noreferrer">FEMA 100 Year Flood Plain</a> </li>
<li><a href="https://geodata.colorado.gov/datasets/COOIT::metropolitan-districts/explore" target="_blank" rel="noopener noreferrer">Metropolitan Districts of Colorado</a></li>
<li>Wildfire Hazard Potential <a href="https://www.fs.usda.gov/rds/archive/catalog/RDS-2015-0047-4">Raster Data</a></li>
</ol>
<p><strong>Process</strong></p>
<ol>
<li>Clean fracking well data using <strong>Python</strong> &amp; <strong>Geopandas</strong></li>
<li>Reduce fracking well data, flood plain polygons, metro districts, and wildfire raster to only those in Boulder County using <strong>ArcGIS Pro</strong>.</li>
<li>Create 1/2 mile buffers around fracking wells and flood zones to provide  an omnidirectional sense of distance.</li>
<li>Upload everything to ESRI's <strong>ArcGIS Web Application</strong> service and create the map layout.</li>
<li>Color code fracking wells by their current status (active, plugged, abandoned, ...)</li>
</ol>
<p><strong>Results</strong></p>
<p>The final product was an ArcGIS Web Application hosted at ESRI.  Access the web application, <a href="https://ums.maps.arcgis.com/apps/webappviewer/index.html?id=fbd9b3af41264143aa576135ac56e41e" title="ArcGIS Web Application" target="_blank" rel="noopener noreferrer">here</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Air Quality in Colorado</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/air-quality-in-colorado/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/air-quality-in-colorado/</id>
            <category term="Science"/>
            <category term="All Projects"/>

        <updated>2024-12-31T07:50:44-07:00</updated>
            <summary type="html">
                <![CDATA[
                    Concept This project sought to shed light on how air quality varies in Colorado over space and time. The goal was to perform statistical and machine learning analyses on air quality data from both mountainous regions and what we locally call the "Front Range" which&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project sought to shed light on how air quality varies in Colorado over space and time. The goal was to perform statistical and machine learning analyses on air quality data from both mountainous regions and what we locally call the "Front Range" which is the urban region on the east foothills of the Rocky Mountains.</p>
<p>Unfortunately, the only air quality monitor in the mountains was only active for a few years and was only intermittently functional when it was active.  Thus, the project pivoted to understanding how the Front Range air quality varies throughout the year.</p>
<p><strong>Data</strong></p>
<p>The dataset used for this project included 6 hour mean &amp; max particle measurements and 6 hour Air Quality Index (AQI) values for four air pollutants including nitrogen dioxide, carbon monoxide, sulfur dioxide, and ozone. There were 35,000 observations from Colorado taken at three different locations.</p>
<p><strong>Process</strong></p>
<p>Using <strong>R</strong>, the following process was used to understand the relationship of the pollutants to one another and how they vary with time:</p>
<ol>
<li>Clean data by removing observations from outside of Colorado.</li>
<li>Perform a correlation analysis using the "pairs" function in the praznik library to see how the pollutants relate to one another.</li>
<li>Factor records by county, AQI readings as either high or low based on guidance from Airnow.gov, and the season based on when the readings were taken.</li>
<li>Use the apriori library to discern association rules. The resulting rules show when each pollutant is generally high or low throughout the year.</li>
<li>Use the silhouette method to determine best number of clusters (2 in this case) for performing a k-means clustering analysis.<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/2/adamsSilhouette.png" alt="" width="500" height="356" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-2xl.png 1920w"></figure></li>
<li>Perform k-means clustering on AQI values for each day of observations in the dataset</li>
<li>Count the number of times each cluster occurs by month.</li>
</ol>
<p><strong>Results</strong></p>
<p>The three analyses performed each confirmed the relationships of the pollutants to one another and through time through triangulation. This project illustrated the relationship between ozone, nitrogen dioxide, and temperature. More specifically, it shows that ozone is highest in the summer when the ambient temperatures are highest and when ozone is high, nitrogen dioxide is generally low. This makes sense given that ozone is created by the combination of nitrogen dioxide, heat, and volatile organic compounds.</p>
<p>Cluster breakdown by mean pollutant value in Adam's County, Colorado (highlighting indicating defining pollutant levels by cluster):</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/2/adams_cluster_values.png" alt="" width="372" height="147" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-2xl.png 1920w"></figure>
<p>Cluster distribution in Adam's County by month:</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/2/adamsClusters2.png" alt="" width="810" height="458" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-2xl.png 1920w"></figure>
<p>The mean pollutant values show that ozone is highest in cluster 1, when nitrogen dioxide is low. The distribution plot shows cluster 1 making up the majority of days during summer months. Then in winter, cluster 2 becomes dominant. Thus, ozone is highest in the summer when it gets hotter. When it temperatures cool off ozone levels go down and nitrogen dioxide levels are higher.</p>
<p>Here is my R code for this project on <a href="https://github.com/mattwilliams-ds/colorado-air-pollution" title="Colorado Air Quality" target="_blank" rel="noopener noreferrer">github</a>.</p>
            ]]>
        </content>
    </entry>
</feed>
