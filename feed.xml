<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>matt williams</title>
    <link href="https://mattwilliams-ds.github.io/gh-page/feed.xml" rel="self" />
    <link href="https://mattwilliams-ds.github.io/gh-page" />
    <updated>2025-03-04T18:29:53-07:00</updated>
    <author>
        <name>Matt Williams</name>
    </author>
    <id>https://mattwilliams-ds.github.io/gh-page</id>

    <entry>
        <title>Sales Analytics using SQL</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/sales-analytics-using-sql/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/sales-analytics-using-sql/</id>
            <category term="Projects"/>
            <category term="Business Intelligence"/>

        <updated>2025-03-02T06:51:11-07:00</updated>
            <summary>
                <![CDATA[
                    Concept Using company sales data and SQLite, I set out to determine the following: Data Sample Sales Data on Kaggle Process Results Exploring the sales data and developing SQL queries in DB Browser was very straight forward. The most challenging query invovled calculating the average&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>Using company sales data and SQLite, I set out to determine the following:</p>
<ul>
<li>What are best and worst performing products and product lines</li>
<li>Determine where the company's products are selling the best</li>
<li>Identifying the company's best customers</li>
<li>Quantify company performance over time</li>
</ul>
<p><strong>Data</strong></p>
<p><a href="https://www.kaggle.com/datasets/kyanyoga/sample-sales-data" target="_blank" rel="noopener noreferrer">Sample Sales Data on Kaggle</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Create a database from the CSV sales data file using DB Browser.</li>
<li>Craft SQLite queries using aggregate functions, subquerries, and joins to answer eight business intelligence questions related to the insights listed above.</li>
<li>Draw actionable insights from the query results.</li>
</ol>
<p><strong>Results</strong></p>
<p>Exploring the sales data and developing SQL queries in DB Browser was very straight forward. The most challenging query invovled calculating the average sales by month for all data in the dataset.</p>
<p>The challenge here was the two and a half years of data so some months are averaged over three instances and other over two months. I was able to next two levels of sub queries to determine how many of each month is in the dataset and then set out to divide the total sales by the number of months.</p>
<p><a href="https://github.com/mattwilliams-ds/SQL-projects/blob/7a73b0d6894b31c932448e6a9462318bc6d0f70a/Sales_Analytics/sales_project_v02.sql" target="_blank" rel="noopener noreferrer">Click here to see the entire set of queries</a></p>
<p>Here is the final query:</p>
<p> </p>
<div><figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/11/Screenshot-from-2025-03-04-18-25-19.png" alt="SQL Query" width="926" height="771" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-25-19-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-25-19-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-25-19-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-25-19-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-25-19-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-25-19-2xl.png 1920w"></figure></div>
<div> </div>
<div>One great feature of DB Browser is the ability to create plots from your query results right there in the program. There aren't many customization options but for exploring data, its more than adequate. Here's a plot of the average sales by month from the above query.</div>
<div> </div>
<div><figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/11/Screenshot-from-2025-03-04-18-27-31.png" alt="average sales by month" width="909" height="729" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-27-31-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-27-31-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-27-31-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-27-31-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-27-31-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/11/responsive/Screenshot-from-2025-03-04-18-27-31-2xl.png 1920w"></figure></div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Vulnerability Study of Coastal Bridges</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/vulnerability-study-of-coastal-bridges/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/vulnerability-study-of-coastal-bridges/</id>
            <category term="Projects"/>

        <updated>2025-01-19T13:13:22-07:00</updated>
            <summary>
                <![CDATA[
                    Concept This project sought to develop a system of rating coastal counties in the U.S. by the vulnerability of their bridge infrastructure. The scoring system developed awards one level or risk for each of the following factors: By identifying coastal counties with at risk bridge&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project sought to develop a system of rating coastal counties in the U.S. by the vulnerability of their bridge infrastructure. The scoring system developed awards one level or risk for each of the following factors:</p>
<figure class="post__image post__image--right"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/10/median_bridge_age-2.png" alt="Median Bridge Age" width="400" height="226" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/median_bridge_age-2-2xl.png 1920w"></figure>Bridge population is aging</p>
<ul>
<li>Bridge Ratings are in decline</li>
<li>County population is increasing (putting more demand on the bridges)</li>
<li>Frequency of storm events that could impact structure health are increasing</li>
</ul>
<p>By identifying coastal counties with at risk bridge structures, Federal and state transportation agencies can have a clear and concise indicator of where future infrastructure investments may be needed.</p>
<p><strong>Data</strong></p>
<ol>
<li>FHWA's <a href="https://www.fhwa.dot.gov/bridge/nbi.cfm" title="FHWA National Bridge Inventory" target="_blank" rel="noopener noreferrer">National Bridge Inventory</a></li>
<li><a href="https://www.census.gov/data/datasets/time-series/demo/popest/2020s-counties-total.html" title="US Census Data" target="_blank" rel="noopener noreferrer">US Census data</a></li>
<li>NCEI <a href="https://www.ncdc.noaa.gov/stormevents/ftp.jsp" title="Storm Events Database" target="_blank" rel="noopener noreferrer">Storm Events Database</a></li>
</ol>
<p><strong>Process</strong></p>
<ol>
<li>Clean &amp; process Bridge Inventory data in <strong>Python</strong>, filtering out non-coastal counties. Attributes of interest included bridge ratings, year of construction, and geolocation data.</li>
<li>Determine bridge age for all structures in cleaned dataset.</li>
<li>Calculate average bridge rating by taking the geometric mean of the ratings for each bridge component.</li>
<li>Calculate the rate of change of the average bridge ratings by county using a <strong>linear regressor</strong> and taking the slope as the rate of change.</li>
<li>Clean &amp; process US Census population data in <strong>Python</strong>.</li>
<li>Calculate rate of population change over a ten year period.</li>
<li>Clean &amp; process NCEI storm event counts, filtering out event types that are not likely to impact bridge condition (fog for example).</li>
<li>Calculate the rate of change of storm event occurances over time using a <strong>Poisson regressor</strong> and filtering out counties with that are not significant (p-values less than 0.05).</li>
<li>Combine average bridge age, change in average bridge rating, county population, and rate of change of storm events for each county into a single <strong>Pandas</strong> dataframe.</li>
<li>Tally risk factors for each county and export data.</li>
<li>Import county risk tally data into <strong>QGIS</strong> and create <strong>chloropleth maps</strong> to document findings.</li>
</ol>
<p><strong>Results</strong></p>
<p>This study successfully rated each coastal county in the US. Ten counties were found to have a vulnerability score of four meaning that they have all for risk factors; aging infrastructure, declining infrastructure, increasing population, and increasing storm events. The following chloropleth map shows the rating for each county.</p>
<figure class="post__image"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/10/county_vulnerability_scores.png" alt="County Vulnerability Map" width="1546" height="891" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/10/responsive/county_vulnerability_scores-2xl.png 1920w"><figcaption><em>Vulnerability Scores of Coastal U.S. Counties</em></figcaption></figure>
<p>The 10 counties with a vulnerability score of four are:</p>
<table style="border-collapse: collapse; width: 100%; height: 241.75px; border-style: none;" border="0">
<tbody>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">San Diego County, California</td>
<td style="width: 49.943%; height: 48.35px;">Barnstable County, Massachusetts</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Santa Cruz County, California</td>
<td style="width: 49.943%; height: 48.35px;">Douglas County, Oregon</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Fairfield County, Connecticut</td>
<td style="width: 49.943%; height: 48.35px;">Nueces County, Texas</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Escambia County, Florida</td>
<td style="width: 49.943%; height: 48.35px;">James City County, Virginia</td>
</tr>
<tr style="height: 48.35px;">
<td style="width: 49.943%; height: 48.35px;">Lee County, Florida</td>
<td style="width: 49.943%; height: 48.35px;">Snohomish County, Washington</td>
</tr>
</tbody>
</table>
<p>The collection of scripts developed in the course of this project are available <a href="https://github.com/mattwilliams-ds/coastal_bridges" title="Bridge Vulnerability Study" target="_blank" rel="noopener noreferrer">here on github</a>. A full report on the project is also <a href="https://drive.google.com/file/d/1Co_J1ejcm1PjDSXzqr7fspOCN_hlEUKl/view?usp=drive_link" title="Bridge Vulnerability Report" target="_blank" rel="noopener noreferrer">available here</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Patterns in Structurally Deficient Bridges</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/patterns-in-structurally-deficient-bridges/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/patterns-in-structurally-deficient-bridges/</id>
            <category term="Projects"/>

        <updated>2025-01-11T13:50:40-07:00</updated>
            <summary>
                <![CDATA[
                    Concept The impetus of this project was to identify common traits among structurally deficient bridges in the United States. To do this, FHWA's National Bridge Inventory was analyzed with the FPGrowth algorithm which is commonly used for finding patterns in retail shopping carts. Applying FPGrowth&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>The impetus of this project was to identify common traits among structurally deficient bridges in the United States. To do this, FHWA's National Bridge Inventory was analyzed with the FPGrowth algorithm which is commonly used for finding patterns in retail shopping carts.</p>
<p>Applying FPGrowth to the bridge inventory required numeric ratings to be categorized as the FPGrowth algorithm works by making associations between text string values present for each record. Several other attributes were also categorized to make more generic rules.</p>
<p><strong>Data</strong></p>
<ul>
<li>FHWA's <a href="https://www.fhwa.dot.gov/bridge/nbi.cfm" title="FHWA National Bridge Inventory" target="_blank" rel="noopener noreferrer">National Bridge Inventory</a></li>
</ul>
<p><strong>Process</strong></p>
<ol>
<li>Read NBI data into a <strong>PySpark</strong> dataframe.</li>
<li>Clean the data by removing bridges with missing or misformatted ratings as well as bridges that are not structurally deficient.</li>
<li>Remove culverts from the data set as these have a different structural system than conventional bridges.</li>
<li>Select data using PySpark <strong>SQL</strong> Functions and create visualizations using <strong>Matplotlib</strong>.</li>
<li>Categorize bridge age, regional location, structure type, and structural system ratings (deck, superstructure, substructure) as follows:<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/9/bridge_rating_categorization.png" alt="Bridge Rating Categorization" width="363" height="240" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/bridge_rating_categorization-2xl.png 1920w"></figure></li>
<li>Use the <strong>FPGrowth</strong> frequent pattern algorithm to develop association rules.</li>
<li>Review the rules and their support &amp; confidence statistical parameters to determine which are worth highlighting.</li>
<li>Perform additional statistical analyses to identify commonalities common shortcomings of deficient bridges.</li>
</ol>
<p><strong>Results</strong></p>
<p>The most relevant and interesting rules found in the set of structurally deficient bridges are as follows:</p>
<table style="border-collapse: collapse; width: 100%;" border="1">
<tbody>
<tr>
<td class="align-center" style="width: 10.1255%;"><strong>Rule</strong></td>
<td class="align-center" style="width: 29.093%;"><strong>Antecedent</strong></td>
<td class="align-center" style="width: 27.6669%;"><strong>Consequent</strong></td>
<td class="align-center" style="width: 15.2596%;"><strong>Support</strong></td>
<td class="align-center" style="width: 17.8266%;"><strong>Confidence</strong></td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">1</td>
<td style="width: 29.093%;">Deck in Good Condition</td>
<td style="width: 27.6669%;">Substructure in Poor Condition</td>
<td class="align-center" style="width: 15.2596%;">0.73</td>
<td class="align-center" style="width: 17.8266%;">0.43</td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">2</td>
<td style="width: 29.093%;">Superstructure in Good Condition</td>
<td style="width: 27.6669%;">Substructure in Poor Condition</td>
<td class="align-center" style="width: 15.2596%;">0.77</td>
<td class="align-center" style="width: 17.8266%;">0.40</td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">3</td>
<td style="width: 29.093%;">Deck is cast-in-place concrete</td>
<td style="width: 27.6669%;">Deck is in poor condition</td>
<td class="align-center" style="width: 15.2596%;">0.44</td>
<td class="align-center" style="width: 17.8266%;">0.24</td>
</tr>
<tr>
<td class="align-center" style="width: 10.1255%;">4</td>
<td style="width: 29.093%;">Girder bridge type + 1-2 spans long</td>
<td style="width: 27.6669%;">Superstructure is a steel girder type</td>
<td class="align-center" style="width: 15.2596%;">0.75</td>
<td class="align-center" style="width: 17.8266%;">0.25</td>
</tr>
</tbody>
</table>
<p>To give you an idea of what all of this is saying, let's break down the first rule. Starting with the confidence, the rule is telling us that "out of all structurally deficient bridges that have a deck in good condition, 73% of them have a substructure that is in poor condition". The confidence tell us that "43% of all structurally deficient bridges have both a deck that is in good condition and a substructure that is in poor condition".</p>
<p>Note that rules one and two both speak to the substructure being in poor condition. This was one of the major findings of this project. The bridge substructure is the most common structural element in poor condition among structurally deficient bridges. This was confirmed by looking at the percentage of structurally deficient systems in the set of deficient bridges.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/9/poor_structural_elements-2.png" alt="Structural systems in poor conditions." width="744" height="410" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/9/responsive/poor_structural_elements-2-2xl.png 1920w"></figure>
<p>Here is a video presentation I made of this project. You can also have a look through my <a href="https://colab.research.google.com/drive/1s6ki2vER671L1qWRRjx5AuH6KnHHPdXa?usp=drive_link" title="Colab Notebook" target="_blank" rel="noopener noreferrer">Colab Notebook</a>.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/vI6ARHd0JPk?si=QFAVg02O1kB01pre" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Predicting Concrete Strength</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/predicting-concrete-strength/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/predicting-concrete-strength/</id>
            <category term="Projects"/>

        <updated>2025-01-05T09:25:27-07:00</updated>
            <summary>
                <![CDATA[
                    Concept This project is an extension to my project titled Effects of Concrete Mix Design &amp; Aging on Strength. Here, I sought to develop a machine learning model that could predict the 28 day concrete breaking strength based on the sample's mix design. I used&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project is an extension to my project titled <a href="https://mattwilliams-ds.github.io/gh-page/effects-of-concrete-mix-design-and-aging-on-strength/" title="effects of concrete mix on strength" target="_blank" rel="noopener noreferrer">Effects of Concrete Mix Design &amp; Aging on Strength</a>. Here, I sought to develop a machine learning model that could predict the 28 day concrete breaking strength based on the sample's mix design. I used a linear regression model as well as a k-nearest neighbors (kNN) model to predict concete strengths.</p>
<p><strong>Data</strong></p>
<p><a href="https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength" title="UC Irvine concrete compressive data" target="_blank" rel="noopener noreferrer">Concrete Compressive Strength [creative commons] from UC Irvine</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Read data into a <strong>Pandas</strong> dataframe.</li>
<li>Look for &amp; remove records with missing data.</li>
<li>Explore the relationship of the mix ingredients using a <strong>seaborn</strong> correlation matrix and pair plots.</li>
<li>Isolate records for 28 day breaks.</li>
<li>Split the data 85% for training and 15% for testing.</li>
<li>Train &amp; test <strong>scikit-learn</strong> linear regression model.</li>
<li>Run a loop with various k values to score <strong>scikit-learn</strong> kNN models &amp; select an appropriate k value.</li>
<li>Test the kNN model.</li>
<li>Repeat steps 4-8 including the initial breaking strength and age of initial break.</li>
</ol>
<p><strong>Results</strong></p>
<p>The correlation matrix (shown here) highlighted several interesting relationships. Most notably is the negative correlation between water and the superplasticizer additive. Second to that, the cement content and concrete strength showed a positive correlation.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/8/Screenshot-from-2025-01-05-09-50-15.png" alt="seaborn correlation matrix plot" width="600" height="492" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-50-15-2xl.png 1920w"></figure>
<p>After exploring the data a bit, I initially tried the linear regression and a k-nearest neighbors based on solely the mix ingredient concentrations for all 28 day breaks. The results were not spectacular with r<sup>2</sup> values of 0.78 and 0.77 respectively. I then changed my approach and in addition to the mix concentrations, I also included the initial breaking strength and the age of the sample at the first test break for the same mix.</p>
<p>This produced better results, improving model performance to r<sup>2</sup> values of 0.88 and 0.87, and is a more useful model, in my opinion. A model like this could be used by concrete batch plants to get an idea of whether the mix they have used will hit the 28 day strength required by the construction project specifications.</p>
<figure ><figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/8/Screenshot-from-2025-01-05-09-53-39-3.png" alt="Predicted vs Actual 28 Day Breaking Strength" width="560" height="568" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-53-39-3-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-53-39-3-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-53-39-3-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-53-39-3-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-53-39-3-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/8/responsive/Screenshot-from-2025-01-05-09-53-39-3-2xl.png 1920w"></figure>
<figcaption >Results for linear regression model including initial concrete breaking strength and age.</figcaption>
</figure>
<p>The r<sup>2</sup> values could be further improved by using a larger data set, which a batch plant would likely have on hand in the records of their previous concrete mixes and sample tests. In the data set used for this analysis only 182 records with 28 day breaks also included a break prior to 28 days so the sample size is quite small. </p>
<p>Check out the entire project <a href="https://colab.research.google.com/drive/1bcAio17m__zbJOQGmOF5qM6oDf0RPT9t?usp=sharing" title="predicting concrete strength notebook" target="_blank" rel="noopener noreferrer">Colab Notebook</a> for more information.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Effects of Concrete Mix Design &amp; Aging on Strength</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/effects-of-concrete-mix-design-and-aging-on-strength/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/effects-of-concrete-mix-design-and-aging-on-strength/</id>
            <category term="Projects"/>

        <updated>2025-01-04T09:08:24-07:00</updated>
            <summary>
                <![CDATA[
                    Concept Construction industries the world over favor different strength concretes for different applications. This project seeks to discern patterns in concrete mix component concentrations for low, medium, and high strength concretes. The dataset includes breaking strengths for just over 1,000 samples, their mix design, and the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>Construction industries the world over favor different strength concretes for different applications. This project seeks to discern patterns in concrete mix component concentrations for low, medium, and high strength concretes. The dataset includes breaking strengths for just over 1,000 samples, their mix design, and the age at which the sample was tested.</p>
<p>While the dataset contains samples broken at different ages, the standard age for concrete strengths is 28 days. This is the strength at which most concrete structures are designed with. Concrete strength does increase over time beyond 28 days. This increase, however, is generally not counted on by structural engineers and is why this project uses only the breaks made at 28 days.</p>
<p><strong>Data</strong></p>
<p><a href="https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength" title="UC Irvine concrete compressive data" target="_blank" rel="noopener noreferrer">Concrete Compressive Strength [creative commons] from UC Irvine</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Read csv data into a <strong>Pandas</strong> dataframe.</li>
<li>Isolate 28 day breaking strengths.</li>
<li>Classify 28 day records as having high, medium, or low strength.</li>
<li>Using <strong>Seaborn</strong>, create a series of box plots for each mix ingredient for each breaking strength classification.</li>
<li>Review box plots for trends in mix concentrations across the three breaking strength classes.</li>
</ol>
<p><strong>Results</strong></p>
<p>Below are the box plots showing the ranges of mix component concentration from low strength to high strength concrete strengths, all at 28 days.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/7//Screenshot-from-2025-01-04-09-07-03.png" alt="Box Plots of Concrete Mix Designs" width="1000" height="428" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/7//responsive/Screenshot-from-2025-01-04-09-07-03-2xl.png 1920w"></figure>
<p>While there is a lot of information in the box plots above it is the level of detail a structural engineer would like to see.  Going by component, the cement content increases with strength. This makes sense as cement is the binding agent that holds the aggregates together. As the cement content increases it displaces and thus reduces the aggregate content. Finally, the water content descreases slightly while superplasticizers increased. This too makes sense as superplasticizers are used to reduce the amount of water in the mix while maintaining the ability to work and consolidate the concrete.</p>
<p>The entire project can be viewed in this <a href="https://drive.google.com/file/d/1gwA8R5vjOfh__89Og_c6CyTE0Oba2PLv/view?usp=sharing" target="_blank" rel="noopener noreferrer">Colab Notebook</a>. It also includes some additional analysis to understand strength gains after 28 days as well. The study found that, on average, concrete strength increases by 21% from day 28 to day 365. Again, this is not generally relied upon in design but is useful to know as it provides an additional factor of safety on the design.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Tornadoes in Tableau</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/tornadoes-in-tableau/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/tornadoes-in-tableau/</id>
            <category term="Projects"/>

        <updated>2025-01-03T07:00:08-07:00</updated>
            <summary>
                <![CDATA[
                    Concept This project was done as part of a data visualization class. My goal with this project was to create dashboards and visualizations in Tableau to show the recent history and impact of tornadoes in the United States. Data NOAA's Storm Events Database Process Results&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project was done as part of a data visualization class. My goal with this project was to create dashboards and visualizations in Tableau to show the recent history and impact of tornadoes in the United States.</p>
<p><strong>Data</strong></p>
<p><a href="https://www.ncdc.noaa.gov/stormevents/" title="NOAA Storm Events Database" target="_blank" rel="noopener noreferrer">NOAA's Storm Events Database</a></p>
<p><strong>Process</strong></p>
<ol>
<li>Reduce storm data to only tornado events.</li>
<li>Clean tornado event data in a <strong>spreadsheet</strong>.</li>
<li>Consolidate storm data to a single event (one storm may be represented as multiple events in the database as it traverses county boundaries)</li>
<li>Develop data visualizations in <strong>Tableau</strong>.</li>
</ol>
<p><strong>Results</strong></p>
<p>This project covers a lot of ground and to communicate all of that several visualizations have been created in Tableau, including a dashboard with a map of all of the tornadoes in this study. Below is a demonstration of my Tableau dashboard and interactive data visualizations.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/R4Rs-IW-sno" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Wireless Sensor Network</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/wireless-sensor-network/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/wireless-sensor-network/</id>
            <category term="Projects"/>

        <updated>2025-01-01T11:36:40-07:00</updated>
            <summary>
                <![CDATA[
                    Concept An Arduino Uno was used to run a sensor for collecting ambient temperature and humidty readings as well. This data was also output to a serial port that fed into a laptop. Hardware Process Resuts This sensor network developed for this project worked as&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<figure class="post__image post__image--right"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/5/wireless_sensor_network.png" alt="" width="550" height="413" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/5/responsive/wireless_sensor_network-2xl.png 1920w"></figure>The purpose of this project was to develop a three node wireless sensor network with an end node, router, and coordinator. Soil moisture sensors were attached to the end node and router. The data flows from the end node to the router and then from the router to the coordinator. The coordinator would then write all data to a serial port where the laptop would listen and record the incoming data.</p>
<p>An Arduino Uno was used to run a sensor for collecting ambient temperature and humidty readings as well. This data was also output to a serial port that fed into a laptop.</p>
<p><strong>Hardware</strong></p>
<ol>
<li>DIGI XBee3 Zigbee Radio Development Boards</li>
<li>Soil Moisture Sensors</li>
<li>Ardiuno Uno</li>
<li>DHT11 Temperature &amp; Humidity Sensor</li>
</ol>
<p><strong>Process</strong></p>
<ol>
<li>Develop <strong>MicroPython</strong> scripts to establish communication &amp; run sensor data collection on the XBee3 radios.</li>
<li>Develop an <strong>Arduino</strong> script to collect ambient temperature &amp; humidity readings.</li>
<li>Develop a <strong>Python</strong> program that listens to local laptop ports for incoming data from the XBee3 coordinator and the Arduino Uno.</li>
<li>Record all data on the laptop to a CSV file for post processing.</li>
</ol>
<p><strong>Resuts</strong></p>
<p>This sensor network developed for this project worked as intended. Using try/except clauses I was able to make the network self-healing. Whenever a node dropped off the network, it would restart the communication protocol to locate the node it is supposed to report to. The MicroPython, Arduino, and serial listening scripts are <a href="https://github.com/mattwilliams-ds/sensor-network" target="_blank" rel="noopener noreferrer">available on Github here</a>. Also, check out a demonstration of the network below.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube.com/embed/Yaj0-c3g5m0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Exoplanets &amp; Space Telescopes</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/exoplanets-and-space-telescopes/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/exoplanets-and-space-telescopes/</id>
            <category term="Projects"/>

        <updated>2025-01-01T08:03:59-07:00</updated>
            <summary>
                <![CDATA[
                    Concept This was an exploratory project that sought to synthesize the search for exoplanets by looking at the planets discovered, their size, and their distance from earth as well as the telescope facilities used to search for exoplanets. The project involves performing statistical analyses on&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This was an exploratory project that sought to synthesize the search for exoplanets by looking at the planets discovered, their size, and their distance from earth as well as the telescope facilities used to search for exoplanets. The project involves performing statistical analyses on the planet data as well as using regular expressions to process text descriptions of the telescopes to discern their size.</p>
<p><strong>Data</strong></p>
<ul>
<li>Nasa's <a href="https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&amp;config=PS" title="Nasa Exoplanet Archive" target="_blank" rel="noopener noreferrer">Exoplanet Archive</a></li>
</ul>
<p><strong>Process</strong></p>
<ol>
<li>Clean &amp; prepare data in <strong>Python</strong> using <strong>pandas</strong>.</li>
<li>Create a histogram of exoplanet discoveries per year with <strong>Matplotlib</strong></li>
<li>Analyze discoveries by method and discoveries by telescope.</li>
<li>Use <strong>RegEx</strong> expression on telescope descriptions to discern the smallest &amp; largest telescopes.</li>
<li>Create scatter plot of planet size by year discovered.</li>
<li>Visualize planet distance from earth by year discovered.</li>
</ol>
<p><strong>Results</strong></p>
<p>Through the course of this project, I found that the Kepler telescope is responsible for the vast majority of exoplanet discoveries. K2 is a second deployment of the Kepler telescope, making it the most productive facility.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/4/telescopes.png" alt="Exoplanet Discoveries by Facility" width="650" height="357" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/4/responsive/telescopes-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/4/responsive/telescopes-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/4/responsive/telescopes-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/4/responsive/telescopes-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/4/responsive/telescopes-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/4/responsive/telescopes-2xl.png 1920w"></figure>
<p>The telescopes used to look for exoplanets have a staggaring range of focal lengths with the smallest being the KELT 80mm telescope and the largest being the 305m Arecibo telescope (no longer in service).</p>
<p>Check out the entire project in my <a href="https://www.kaggle.com/code/mattwilliamsds/the-search-for-exoplanets" title="exoplanet notebook" target="_blank" rel="noopener noreferrer">kaggle notebook</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Assessing Homebuyer&#x27;s Risk</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/assessing-homebuyers-risk/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/assessing-homebuyers-risk/</id>
            <category term="Projects"/>

        <updated>2024-12-31T15:43:33-07:00</updated>
            <summary>
                <![CDATA[
                    A web application for evaluating property risks in Boulder County, Colorado. Concept The web map, created with ESRI's ArcGIS Web Application, allows homebuyers and residents to locate a property and see nearby risks including fracking wells, FEMA flood zones, Metro taxing district boundaries, as well&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>A web application for evaluating property risks in Boulder County, Colorado.</p>
<p><strong>Concept</strong></p>
<figure class="post__image post__image--right"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/3/webapp-2.png" alt="" width="450" height="356" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/3/responsive/webapp-2-2xl.png 1920w"></figure>There are a lot of tools for assessing a homes proximity to schools, community ammenities, parks, and more. What is harder to come by are tools for assessing potential risks to a home you may wish to purchase. This project seeks to solve that problem for Boulder County, Colorado.</p>
<p>The web map, created with ESRI's ArcGIS Web Application, allows homebuyers and residents to locate a property and see nearby risks including fracking wells, FEMA flood zones, Metro taxing district boundaries, as well as wildfire risk. Users of the application can then gauge their own cofort level with the proximity to these risk factors.</p>
<p><strong>Data</strong></p>
<ol>
<li>Fracking well locations and current status from <a href="https://www.fractracker.org/" target="_blank" rel="noopener noreferrer">FracTracker Alliance</a></li>
<li><a href="https://geo.colorado.edu/catalog/47540-5ca228ffd43267000b8c7448" target="_blank" rel="noopener noreferrer">FEMA 100 Year Flood Plain</a> </li>
<li><a href="https://geodata.colorado.gov/datasets/COOIT::metropolitan-districts/explore" target="_blank" rel="noopener noreferrer">Metropolitan Districts of Colorado</a></li>
<li>Wildfire Hazard Potential <a href="https://www.fs.usda.gov/rds/archive/catalog/RDS-2015-0047-4">Raster Data</a></li>
</ol>
<p><strong>Process</strong></p>
<ol>
<li>Clean fracking well data using <strong>Python</strong> &amp; <strong>Geopandas</strong></li>
<li>Reduce fracking well data, flood plain polygons, metro districts, and wildfire raster to only those in Boulder County using <strong>ArcGIS Pro</strong>.</li>
<li>Create 1/2 mile buffers around fracking wells and flood zones to provide  an omnidirectional sense of distance.</li>
<li>Upload everything to ESRI's <strong>ArcGIS Web Application</strong> service and create the map layout.</li>
<li>Color code fracking wells by their current status (active, plugged, abandoned, ...)</li>
</ol>
<p><strong>Results</strong></p>
<p>The final product was an ArcGIS Web Application hosted at ESRI.  Access the web application, <a href="https://ums.maps.arcgis.com/apps/webappviewer/index.html?id=fbd9b3af41264143aa576135ac56e41e" title="ArcGIS Web Application" target="_blank" rel="noopener noreferrer">here</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Air Quality in Colorado</title>
        <author>
            <name>Matt Williams</name>
        </author>
        <link href="https://mattwilliams-ds.github.io/gh-page/air-quality-in-colorado/"/>
        <id>https://mattwilliams-ds.github.io/gh-page/air-quality-in-colorado/</id>
            <category term="Projects"/>

        <updated>2024-12-31T07:50:44-07:00</updated>
            <summary>
                <![CDATA[
                    Concept This project sought to shed light on how air quality varies in Colorado over space and time. The goal was to perform statistical and machine learning analyses on air quality data from both mountainous regions and what we locally call the "Front Range" which&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><strong>Concept</strong></p>
<p>This project sought to shed light on how air quality varies in Colorado over space and time. The goal was to perform statistical and machine learning analyses on air quality data from both mountainous regions and what we locally call the "Front Range" which is the urban region on the east foothills of the Rocky Mountains.</p>
<p>Unfortunately, the only air quality monitor in the mountains was only active for a few years and was only intermitently functional when it was active.  Thus, the project pivotted to understanding how the Front Range air quality varies throughout the year.</p>
<p><strong>Data</strong></p>
<p>The dataset used for this project included 6 hour mean &amp; max particle measurements and 6 hour Air Quality Index (AQI) values for four air pollutants including nitrogen dioxide, carbon monoxide, sulfur dioxide, and ozone. There were 35,000 observations from Colorado taken at three different locations.</p>
<p><strong>Process</strong></p>
<p>Using <strong>R</strong>, the following process was used to understand the relationship of the pollutants to one another and how they vary with time:</p>
<ol>
<li>Clean data by removing observations from outside of Colorado.</li>
<li>Perform a correlation analysis using the "pairs" function in the praznik library to see how the pollutants relate to one another.</li>
<li>Factor records by county, AQI readings as either high or low based on guidance from Airnow.gov, and the season based on when the readings were taken.</li>
<li>Use the apriori library to discern association rules. The resulting rules show when each pollutant is generally high or low throughout the year.</li>
<li>Use the silhouette method to determine best number of clusters (2 in this case) for performing a k-means clustering analysis.<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/2/adamsSilhouette.png" alt="" width="500" height="356" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsSilhouette-2xl.png 1920w"></figure></li>
<li>Perform k-means clustering on AQI values for each day of observations in the dataset</li>
<li>Count the number of times each cluster occurs by month.</li>
</ol>
<p><strong>Results</strong></p>
<p>The three analyses performed each confirmed the relationships of the pollutants to one another and through time through triangulation. This project illustrated the relationship between ozone, nitrogen dioxide, and temperature. More specifically, it shows that ozone is highest in the summer when the ambient temperatures are highest and when ozone is high, nitrogen dioxide is generally low. This makes sense given that ozone is created by the combination of nitrogen dioxide, heat, and volatile organic compounds.</p>
<p>Cluster breakdown by mean pollutant value in Adam's County, Colorado (highlighting indicating defining pollutant levels by cluster):</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/2/adams_cluster_values.png" alt="" width="372" height="147" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adams_cluster_values-2xl.png 1920w"></figure>
<p>Cluster distribution in Adam's County by month:</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://mattwilliams-ds.github.io/gh-page/media/posts/2/adamsClusters2.png" alt="" width="810" height="458" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-xs.png 640w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-sm.png 768w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-md.png 1024w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-lg.png 1366w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-xl.png 1600w ,https://mattwilliams-ds.github.io/gh-page/media/posts/2/responsive/adamsClusters2-2xl.png 1920w"></figure>
<p>The mean pollutant values show that ozone is highest in cluster 1, when nitrogen dioxide is low. The distribution plot shows cluster 1 making up the majority of days during summer months. Then in winter, cluster 2 becomes dominant. Thus, ozone is highest in the summer when it gets hotter. When it temperatures cool off ozone levels go down and nitrogen dioxide levels are higher.</p>
<p>Here is my R code for this project on <a href="https://github.com/mattwilliams-ds/colorado-air-pollution" title="Colorado Air Quality" target="_blank" rel="noopener noreferrer">github</a>.</p>
            ]]>
        </content>
    </entry>
</feed>
